{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.4)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboardX) (3.17.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboardX) (1.19.5)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardX) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HDAX91xcOA2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tensorboardX import SummaryWriter\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "manualSeed = 999\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "# drive.mount(\"/content/gdrive\")\n",
    "path = \"/home/ec2-user/SageMaker/PyTorch-AdaIN-StyleTransfer\"\n",
    "\n",
    "\n",
    "# sys.path.append(path)\n",
    "from Utils import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBxpcfaNybWC"
   },
   "outputs": [],
   "source": [
    "# Images Dataset that returns one style and one content image. As I only trained using 40.000\n",
    "# images each, each image is randomly sampled. The way it is implemented does not allow multi-threading. However\n",
    "# as this network is relatively small and training times low, no improved class was implemented.\n",
    "\n",
    "class Images(Dataset): \n",
    "  def __init__(self, root_dir1, root_dir2, transform=None):\n",
    "    self.root_dir1 = root_dir1\n",
    "    self.root_dir2 = root_dir2\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return min(len(os.listdir(self.root_dir1)), len(os.listdir(self.root_dir2)))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    all_names1, all_names2 = os.listdir(self.root_dir1), os.listdir(self.root_dir2)\n",
    "    idx1, idx2 = np.random.randint(0, len(all_names1)), np.random.randint(0, len(all_names2))\n",
    "\n",
    "    img_name1, img_name2 = os.path.join(self.root_dir1, all_names1[idx1]), os.path.join(self.root_dir2, all_names2[idx2])\n",
    "    image1 = Image.open(img_name1).convert(\"RGB\")\n",
    "    image2 = Image.open(img_name2).convert(\"RGB\")\n",
    "\n",
    "    if self.transform:\n",
    "      image1 = self.transform(image1)\n",
    "      image2 = self.transform(image2)\n",
    "\n",
    "    return image1, image2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MIVw4MQo6WKu"
   },
   "outputs": [],
   "source": [
    "# To note is that the images are not normalised\n",
    "transform = transforms.Compose([transforms.Resize(512), \n",
    "                               transforms.CenterCrop(256),\n",
    "                               transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# Specify the path to the style and content images\n",
    "pathStyleImages = \"data/wikiart\"\n",
    "pathContentImages = \"data/coco_images\" \n",
    "\n",
    "\n",
    "all_img = Images(pathStyleImages, pathContentImages, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGd2_znLxLIy"
   },
   "outputs": [],
   "source": [
    "# Simple save \n",
    "def save_state(decoder, optimiser, iters, run_dir):\n",
    "  \n",
    "  name = \"models/StyleTransfer_Checkpoint_Iter_{}.tar\".format(iters)\n",
    "  torch.save({\"Decoder\" : decoder,\n",
    "              \"Optimiser\" : optimiser,\n",
    "              \"iters\": iters\n",
    "              }, os.path.join(path, name))\n",
    "  print(\"Saved : {} succesfully\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1600861736262,
     "user": {
      "displayName": "Marvin Alberts",
      "photoUrl": "",
      "userId": "07281461137485771510"
     },
     "user_tz": -120
    },
    "id": "nNf9XPp2HgCO"
   },
   "outputs": [],
   "source": [
    "def training_loop(network, # StyleTransferNetwork\n",
    "                  dataloader_comb, # DataLoader\n",
    "                  n_epochs, # Number of Epochs\n",
    "                  run_dir # Directory in which the checkpoints and tensorboard files are saved\n",
    "                  ):\n",
    "  \n",
    "\n",
    "  writer = SummaryWriter(os.path.join(path, run_dir))\n",
    "  # Fixed images to compare over time\n",
    "  fixed_batch_style, fixed_batch_content = all_img[0]\n",
    "  fixed_batch_style, fixed_batch_content =  fixed_batch_style.unsqueeze(0).to(device), fixed_batch_content.unsqueeze(0).to(device) # Move images to device\n",
    "\n",
    "  writer.add_image(\"Style\", torchvision.utils.make_grid(fixed_batch_style))\n",
    "  writer.add_image(\"Content\", torchvision.utils.make_grid(fixed_batch_content))\n",
    "\n",
    "  iters = network.iters\n",
    "\n",
    "  for epoch in range(1, n_epochs+1):\n",
    "    tqdm_object = tqdm(dataloader_comb, total=len(dataloader_comb))\n",
    "\n",
    "    for style_imgs, content_imgs in tqdm_object:\n",
    "      network.adjust_learning_rate(network.optimiser, iters)\n",
    "      style_imgs = style_imgs.to(device)\n",
    "      content_imgs = content_imgs.to(device)\n",
    "\n",
    "      loss_comb, content_loss, style_loss = network(style_imgs, content_imgs)\n",
    "\n",
    "      network.optimiser.zero_grad()\n",
    "      loss_comb.backward()\n",
    "      network.optimiser.step()\n",
    "\n",
    "      # Update status bar, add Loss, add Images\n",
    "      tqdm_object.set_postfix_str(\"Combined Loss: {:.3f}, Style Loss: {:.3f}, Content Loss: {:.3f}\".format(\n",
    "                                  loss_comb.item()*100, style_loss.item()*100, content_loss.item()*100))\n",
    "    \n",
    "      if iters % 25 == 0:\n",
    "        writer.add_scalar(\"Combined Loss\", loss_comb*1000, iters)\n",
    "        writer.add_scalar(\"Style Loss\", style_loss*1000, iters)\n",
    "        writer.add_scalar(\"Content Loss\", content_loss*1000, iters)\n",
    "\n",
    "      if (iters+1) % 2000 == 1:\n",
    "        with torch.no_grad():\n",
    "          network.set_train(False)\n",
    "          images = network(fixed_batch_style, fixed_batch_content)\n",
    "          img_grid = torchvision.utils.make_grid(images)\n",
    "          writer.add_image(\"Progress Iter: {}\".format(iters), img_grid)\n",
    "          network.set_train(True)\n",
    "\n",
    "      if (iters+1) % 4000 == 1:\n",
    "          save_state(network.decoder.state_dict(), network.optimiser.state_dict(), iters, run_dir)\n",
    "          writer.close()\n",
    "          writer = SummaryWriter(os.path.join(path, run_dir))\n",
    "\n",
    "      iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKg_C0K6leFp"
   },
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 1e-4\n",
    "learning_rate_decay = 5e-5\n",
    "\n",
    "dataloader_comb = DataLoader(all_img, batch_size=5, shuffle=True, num_workers=0, drop_last=True)\n",
    "gamma = torch.tensor([2.0]).to(device) # Style weight. Gamma = 2\n",
    "\n",
    "n_epochs = 5\n",
    "run_dir = \"runs/Run_1\" # Change if you want to save the checkpoints/tensorboard files in a different directory\n",
    "\n",
    "state_encoder = torch.load(os.path.join(path, \"data/resources/vgg_normalised.pth\"))\n",
    "network = networks.StyleTransferNetwork(device,\n",
    "                                        state_encoder,\n",
    "                                        learning_rate,\n",
    "                                        learning_rate_decay,\n",
    "                                        gamma,\n",
    "                                        load_fromstate=False,\n",
    "                                        load_path=os.path.join(path, \"models/StyleTransfer_Checkpoint_Iter_120000.tar\"),\n",
    "                                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_huZgs0ClUJ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fea807d7284b57b10c4e10b032c97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16019 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved : models/StyleTransfer_Checkpoint_Iter_0.tar succesfully\n",
      "Saved : models/StyleTransfer_Checkpoint_Iter_4000.tar succesfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py:2858: DecompressionBombWarning: Image size (99962094 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved : models/StyleTransfer_Checkpoint_Iter_8000.tar succesfully\n"
     ]
    }
   ],
   "source": [
    "training_loop(network, dataloader_comb, n_epochs, run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4m-uIZxqXeoD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Style Transfer Train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
